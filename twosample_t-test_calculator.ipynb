{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B testing: 2 sample t-test calculator \n",
    "__Author: Laura Urbisci__   \n",
    "\n",
    "---\n",
    "## Overview\n",
    "\n",
    "The goal is to automate the 2 sample t-test in python. While python has a t-test function (i.e., `ttest_ind`) in the scipy package, it does not give the user an option to specify if this is a two- or one-sided test. In addition, one still needs to do an F-test for equal variance before the 2 sample t-test. Thus, I will roughly base my function off of R's `t.test` (See documentation [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) for the Python package and [here](https://www.rdocumentation.org/packages/stats/versions/3.6.0/topics/t.test) for R package). The end product is a python function where the user can input the raw data for the two samples, the alternative hypothesis, and significance level and determine if the average value differs across the two groups.\n",
    "\n",
    "*Note: when using t-tests, we assume the data (aka samples) come from a Normal distribution. If the data is not Normallly distributed a non-parametric test is preferred.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.598438418230809\n",
      "7\n",
      "9\n",
      "0.020170487653488944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "s1 = [64.2, 28.4, 85.3, 83.1, 13.4, 56.8, 44.2, 90]\n",
    "s2 = [45, 29.5, 32.3, 49.3, 18.3, 34.2, 43.9, 13.8, 27.4, 43.4]\n",
    "\n",
    "# f-test for equal variance, two-sided test\n",
    "var_s1 = np.var(s1, ddof = 1) # need ddof so divisor is N-1\n",
    "var_s2 = np.var(s2, ddof = 1) # need ddof so divisor is N-1\n",
    "f_stat = var_s1/var_s2 \n",
    "print(f_stat) # F statistic\n",
    "m = len(s1)\n",
    "n = len(s2)\n",
    "print(m-1) # num df\n",
    "print(n-1) # denom df\n",
    "print( 2*(1-stats.f.cdf(f_stat, dfn=m-1, dfd=n-1)) ) # p-value from f_stat\n",
    "2*(1-stats.f.cdf(f_stat, dfn=m-1, dfd=n-1)) < 0.05 # reject null that same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3102761451054765\n",
      "9.000549141935105\n",
      "0.046213934265318946\n",
      "0.9768930328673405\n",
      "0.023106967132659473\n"
     ]
    }
   ],
   "source": [
    "# two sample t-test, unequal variance\n",
    "\n",
    "# two-sided test\n",
    "t_stat_un = (np.mean(s1) - np.mean(s2))/np.sqrt( (var_s1/m) + (var_s2/n) )\n",
    "print(t_stat_un) # t statistic\n",
    "df_t_un = ( (var_s1/m) + (var_s2/n) )**2/( (var_s1/m)**2/(m-1) + (var_s2/n)**2/(n-1) ) \n",
    "print(df_t_un) # df\n",
    "print( 2*(1-stats.t.cdf(t_stat_un, df=df_t_un)) ) # two-sided p-value from t_test_un\n",
    "2*(1-stats.t.cdf(t_stat_un, df=df_t_un)) < 0.05\n",
    "\n",
    "# one-sided test, less\n",
    "print(stats.t.cdf(t_stat_un, df=df_t_un)) # one-sided p-value from t_test_un\n",
    "\n",
    "# one-sided test, greater\n",
    "print(1-stats.t.cdf(t_stat_un, df=df_t_un)) # one-sided p-value from t_test_un\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5098649986861177\n",
      "16\n",
      "0.023208765109723917\n",
      "0.988395617445138\n",
      "0.011604382554861958\n"
     ]
    }
   ],
   "source": [
    "# two sample t-test, equal variance\n",
    "\n",
    "# two-sided test\n",
    "s_pool = (((m-1)*var_s1) + ((n-1)*var_s2))/(m+n-2)\n",
    "t_stat = (np.mean(s1) - np.mean(s2))/(np.sqrt(s_pool)*np.sqrt( (1/m) + (1/n) ))\n",
    "print(t_stat) # t statistic\n",
    "df_t = m+n-2\n",
    "print(df_t) # df\n",
    "print( 2*(1-stats.t.cdf(t_stat, df=df_t)) ) # two-sided p-value from t_test\n",
    "2*(1-stats.t.cdf(t_stat, df=df_t)) < 0.05\n",
    "\n",
    "# one-sided test, less\n",
    "print(stats.t.cdf(t_stat, df=df_t)) # one-sided p-value from t_test_un\n",
    "\n",
    "# one-sided test, greater\n",
    "print(1-stats.t.cdf(t_stat, df=df_t)) # one-sided p-value from t_test_un\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final product\n",
    "\n",
    "A python function that takes the following inputs:\n",
    "- Sample 1 raw data (s1)\n",
    "- Sample 2 raw data (s2)\n",
    "- Specify alternative hypothesis (alternative): two-sided (default), less, greater\n",
    "- Significance level (sig_level): 5% is default\n",
    "\n",
    "\n",
    "and outputs the p-value and verdict of the 2 sample t-test. \n",
    "\n",
    "*Note: In this function, we assume the data follow a Normal distribution. When this assumption is not met, this calculator should not be used.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def twosample_t_test(s1, s2, alternative=\"two_sided\", sig_level=0.05):\n",
    "    \"\"\"\n",
    "    Returns the p-value and verdict a 2 sample t-test\n",
    "    \n",
    "    Inputs:\n",
    "    s1(array): sample 1 raw data\n",
    "    s2(array): sample 2 raw data\n",
    "    alternative(string): specify the alternative hypothesis, must be \"two_sided\" (default),\n",
    "    \"greater\", or \"less\"\n",
    "    sig_level: significance level, default is 5%\n",
    "    \n",
    "    Output:\n",
    "    verdict with p-value\n",
    "    \"\"\"\n",
    "    # f-test for equal variance, two-sided test\n",
    "    var_s1 = np.var(s1, ddof = 1) # need ddof so divisor is N-1\n",
    "    var_s2 = np.var(s2, ddof = 1) # need ddof so divisor is N-1\n",
    "    f_stat = var_s1/var_s2 \n",
    "    m = len(s1)\n",
    "    n = len(s2)\n",
    "    f_pval = 2*(1-stats.f.cdf(f_stat, dfn=m-1, dfd=n-1))  # p-value from f_stat\n",
    "    \n",
    "    if f_pval >= sig_level:\n",
    "        # fail to reject null -> use two sample t-test with equal variance\n",
    "        s_pool = (((m-1)*var_s1) + ((n-1)*var_s2))/(m+n-2)\n",
    "        # t statistic\n",
    "        t_stat = (np.mean(s1) - np.mean(s2))/(np.sqrt(s_pool)*np.sqrt( (1/m) + (1/n) ))\n",
    "        # df \n",
    "        df_t = m+n-2\n",
    "        \n",
    "        # alternative hypothesis:\n",
    "        if alternative == \"two_sided\":\n",
    "            # two-sided test p-value\n",
    "            p_2side = 2*(1 - stats.t.cdf(abs(t_stat), df=df_t))\n",
    "            \n",
    "            # verdict\n",
    "            if p_2side >= sig_level:\n",
    "                print(\"Verdict: No significant difference in means (p={})\".format(round(p_2side,4)))\n",
    "            else:\n",
    "                print(\"Verdict: Significant difference in means(p={})\".format(round(p_2side,4)))\n",
    "            \n",
    "        elif alternative == \"less\":\n",
    "            # one-sided test (less) p-value\n",
    "            p_less = 1- stats.t.cdf(abs(t_stat), df=df_t)\n",
    "            \n",
    "            # verdict\n",
    "            if p_less >= sig_level:\n",
    "                print(\"Verdict: No significant difference (p={})\".format(round(p_less,4)))\n",
    "            else:\n",
    "                print(\"Verdict: Significant difference (p={}). Sample 1's mean is less than sample 2's mean\".format(round(p_less,4)))\n",
    "         \n",
    "        else:\n",
    "            # one-sided test (greater) p-value  \n",
    "            p_great = stats.t.cdf(abs(t_stat), df=df_t)\n",
    "            \n",
    "            # verdict\n",
    "            if p_great >= sig_level:\n",
    "                print(\"Verdict: No significant difference (p={})\".format(round(p_great,4)))\n",
    "            else:\n",
    "                print(\"Verdict: Significant difference (p={}). Sample 1's mean is greater than sample 2's mean\".format(round(p_great,4)))\n",
    "   \n",
    "    else:\n",
    "        # reject null that same -> use two sample t-test with unequal variance\n",
    "        # t statistic\n",
    "        t_stat_un = (np.mean(s1) - np.mean(s2))/np.sqrt( (var_s1/m) + (var_s2/n) )\n",
    "        # df\n",
    "        df_t_un = ( (var_s1/m) + (var_s2/n) )**2/( (var_s1/m)**2/(m-1) + (var_s2/n)**2/(n-1) ) \n",
    "        \n",
    "        # alternative hypothesis:\n",
    "        if alternative == \"two_sided\":\n",
    "            # two-sided test p-value\n",
    "            p_2side_un = 2*(1 - stats.t.cdf(abs(t_stat_un), df=df_t_un))\n",
    "            \n",
    "            # verdict\n",
    "            if p_2side_un >= sig_level:\n",
    "                print(\"Verdict: No significant difference in means (p={})\".format(round(p_2side_un,4)))\n",
    "            else:\n",
    "                print(\"Verdict: Significant difference in means (p={})\".format(round(p_2side_un,4)))\n",
    "          \n",
    "        elif alternative == \"less\":\n",
    "            # one-sided test(less) p-value\n",
    "            p_less_un = 1 - stats.t.cdf(abs(t_stat_un), df=df_t_un)\n",
    "            \n",
    "            # verdict\n",
    "            if p_less_un >= sig_level:\n",
    "                print(\"Verdict: No significant difference (p={})\".format(round(p_less_un,4)))\n",
    "            else:\n",
    "                print(\"Verdict: Significant difference (p={}). Sample 1's mean is less than sample 2's mean\".format(round(p_less_un,4)))\n",
    "            \n",
    "        else:\n",
    "            # one-sided test (greater) p-value  \n",
    "            p_great_un = stats.t.cdf(abs(t_stat_un), df=df_t_un)\n",
    "            \n",
    "            # verdict\n",
    "            if p_great_un >= sig_level:\n",
    "                print(\"Verdict: No significant difference (p={})\".format(round(p_great_un,4)))\n",
    "            else:\n",
    "                print(\"Verdict: Significant difference (p={}). Sample 1's mean is greater than sample 2's mean\".format(round(p_great_un,4)))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verdict: Significant difference in means (p=0.0462)\n",
      "Verdict: Significant difference in means (p=0.0462)\n",
      "Verdict: No significant difference (p=0.9769)\n",
      "Verdict: Significant difference (p=0.0231). Sample 1's mean is greater than sample 2's mean\n"
     ]
    }
   ],
   "source": [
    "s1 = [64.2, 28.4, 85.3, 83.1, 13.4, 56.8, 44.2, 90]\n",
    "s2 = [45, 29.5, 32.3, 49.3, 18.3, 34.2, 43.9, 13.8, 27.4, 43.4]\n",
    "\n",
    "twosample_t_test(s1, s2, alternative=\"two_sided\", sig_level=0.05)\n",
    "twosample_t_test(s1, s2, sig_level=0.05)\n",
    "twosample_t_test(s1, s2, alternative=\"less\", sig_level=0.05)\n",
    "twosample_t_test(s1, s2, alternative=\"greater\", sig_level=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
